{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqYYamI9RTqT"
      },
      "source": [
        "# quantize BGE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install haystack-ai"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "husd3AShtCrN",
        "outputId": "890976b5-e467-49a9-8c4f-19cef9e7a007"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting haystack-ai\n",
            "  Downloading haystack_ai-2.8.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting haystack-experimental (from haystack-ai)\n",
            "  Downloading haystack_experimental-0.4.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.1.4)\n",
            "Collecting lazy-imports (from haystack-ai)\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (10.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (1.57.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.2.2)\n",
            "Collecting posthog (from haystack-ai)\n",
            "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (9.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->haystack-ai) (1.17.0)\n",
            "Collecting monotonic>=1.5 (from posthog->haystack-ai)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->haystack-ai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (2024.12.14)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (2.27.1)\n",
            "Downloading haystack_ai-2.8.0-py3-none-any.whl (391 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m391.4/391.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading haystack_experimental-0.4.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Installing collected packages: monotonic, lazy-imports, backoff, posthog, haystack-experimental, haystack-ai\n",
            "Successfully installed backoff-2.2.1 haystack-ai-2.8.0 haystack-experimental-0.4.0 lazy-imports-0.3.1 monotonic-1.6 posthog-3.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3pspUV6URWW_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from haystack.components.generators import HuggingFaceLocalGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Oq-vSWYRvja",
        "outputId": "c1f9ac61-e2cc-4c16-98cb-34da25f560b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load the BGE small model and tokenizer\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RwQZpl4HRzT_"
      },
      "outputs": [],
      "source": [
        "# Quantize the model to int8\n",
        "model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Move model to CPU (assuming CPU optimization)\n",
        "model = model.to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uw-DHuETR10v"
      },
      "outputs": [],
      "source": [
        "# Load a sample dataset (you can replace this with your own dataset)\n",
        "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFl2ANRDR5gf",
        "outputId": "f07e5307-6744-4080-81fc-43e8c2574e2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding documents: 100%|██████████| 151/151 [00:23<00:00,  6.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded 151 documents.\n",
            "Embedding shape: (384,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Function to embed text\n",
        "def embed_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "# Embed the dataset\n",
        "embeddings = []\n",
        "for item in tqdm(dataset, desc=\"Embedding documents\"):\n",
        "    embedding = embed_text(item['content'])\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "print(f\"Embedded {len(embeddings)} documents.\")\n",
        "print(f\"Embedding shape: {embeddings[0].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yNCufJfzR67_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "prompt_template = \"\"\"\n",
        "\n",
        "You are a helpful AI assistant. You are given contexts and a question.\n",
        "You must answer the question using the information given in the context ONLY.\n",
        "If you don't know the answer say 'I don't know!'.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "def get_relevant_context(question, embeddings, top_k=5):\n",
        "    context_start = time.time()\n",
        "    question_embedding = embed_text(question)\n",
        "    similarities = cosine_similarity([question_embedding], embeddings)\n",
        "    top_indices = similarities.argsort()[0][-top_k:][::-1]\n",
        "    top_context = [dataset[int(i)]['content'] for i in top_indices]\n",
        "    context_time = time.time() - context_start\n",
        "    print(f\"Context retrieval time: {context_time:.4f} seconds\")\n",
        "    return top_context\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_text(question)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hjiVqZ1j3pvO",
        "outputId": "98040838-4a88-4a67-c83b-b6131aa34bfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-5.52479684e-01, -2.10879296e-01,  6.11634910e-01,  1.67584822e-01,\n",
              "        1.58225179e-01, -7.78047293e-02,  4.19106543e-01,  2.70687997e-01,\n",
              "        4.06441092e-02,  2.95371562e-02,  1.24708109e-01, -5.93429923e-01,\n",
              "        1.89952299e-01,  2.54397303e-01, -1.35276005e-01, -4.15784627e-01,\n",
              "        2.98529088e-01,  2.77787626e-01, -7.09237456e-01,  1.44595534e-01,\n",
              "        4.44652647e-01, -2.63073772e-01,  6.01049550e-02, -1.03174555e+00,\n",
              "       -1.69600621e-01,  7.79938161e-01, -1.45345286e-01, -6.67331461e-03,\n",
              "       -2.29361698e-01, -1.60577774e+00,  1.39026135e-01, -4.86054093e-01,\n",
              "        2.69018024e-01, -2.30139166e-01, -6.78300625e-03, -1.57196730e-01,\n",
              "       -7.14094281e-01,  3.94277543e-01, -2.27301821e-01,  1.80117577e-01,\n",
              "        4.27449614e-01,  2.70393878e-01, -2.81613380e-01, -3.63092959e-01,\n",
              "       -3.64223450e-01, -7.72437155e-01, -2.02933982e-01, -5.23339920e-02,\n",
              "        5.06054521e-01,  5.29578030e-02,  4.09504563e-01, -2.11529821e-01,\n",
              "        3.91138464e-01,  1.79280281e-01,  3.91027518e-02,  7.74889588e-01,\n",
              "        5.97319007e-01,  5.16326010e-01,  3.98346871e-01, -9.69588682e-02,\n",
              "        4.49030131e-01,  3.77952158e-01, -1.32690644e+00,  7.07994044e-01,\n",
              "        2.98651874e-01,  5.53661346e-01, -3.81144315e-01, -4.37370509e-01,\n",
              "        1.34625241e-01,  1.52139053e-01,  1.33341894e-01, -1.18652722e-02,\n",
              "        1.68849140e-01,  3.04214478e-01, -4.81996298e-01, -1.46609947e-01,\n",
              "       -9.40278769e-02, -4.16994572e-01, -4.10756953e-02,  5.10076046e-01,\n",
              "       -7.02391684e-01,  4.15447176e-01, -4.45168257e-01,  3.74128610e-01,\n",
              "       -1.21243358e-01, -1.75178513e-01, -2.14054119e-02,  1.51418015e-01,\n",
              "        2.30071530e-01,  7.32076541e-02, -3.86953622e-01, -4.62598592e-01,\n",
              "       -9.56219658e-02,  9.88415405e-02, -4.85542625e-01,  2.30171472e-01,\n",
              "       -1.09917380e-01,  8.86448100e-02, -5.04662208e-02,  1.64047086e+00,\n",
              "       -1.06251881e-01,  3.14682811e-01,  9.65697289e-01, -1.27950042e-01,\n",
              "        9.61814106e-01, -1.11244939e-01,  1.93906218e-01, -4.27390128e-01,\n",
              "       -2.38973603e-01,  2.04671636e-01,  2.26094022e-01, -1.71674360e-02,\n",
              "       -7.51822516e-02, -1.79260626e-01,  5.93686759e-01, -1.08907297e-01,\n",
              "       -3.26393470e-02,  1.07300781e-01, -4.20420945e-01,  3.09445150e-02,\n",
              "       -1.01248898e-01, -1.55098602e-01,  2.13969588e-01, -4.10138190e-01,\n",
              "        1.86360568e-01, -2.52225548e-01,  7.73585618e-01,  6.74499393e-01,\n",
              "       -2.21121222e-01,  2.00760037e-01, -2.81563532e-02, -5.89496076e-01,\n",
              "       -3.14689934e-01, -1.58626363e-01,  3.41705158e-02,  4.87266511e-01,\n",
              "        2.91353852e-01,  3.32797281e-02,  2.15515032e-01,  3.61896664e-01,\n",
              "       -4.31869924e-01, -1.01421511e+00,  5.00931144e-02, -6.63562417e-01,\n",
              "       -1.55727968e-01,  6.58053875e-01, -7.36662805e-01,  1.35696739e-01,\n",
              "       -1.12407878e-01,  3.60557079e-01, -7.97273368e-02,  1.79272637e-01,\n",
              "        4.41603661e-02,  1.27856225e-01, -5.04771471e-02, -3.09565485e-01,\n",
              "       -8.46892670e-02,  1.16895008e+00, -2.52827466e-01,  3.28931548e-02,\n",
              "        3.86134177e-01, -3.63118380e-01, -3.94226015e-01,  4.73414600e-01,\n",
              "        2.90179044e-01, -1.04278600e+00,  3.52473035e-02,  3.55899408e-02,\n",
              "        3.31781328e-01, -5.15257344e-02, -3.01666826e-01,  6.45427883e-01,\n",
              "       -4.35906976e-01,  4.91176516e-01,  1.11023843e+00,  2.81903654e-01,\n",
              "       -8.02501321e-01,  6.07065260e-02,  4.49254699e-02, -1.40440157e-02,\n",
              "       -5.02681017e-01, -1.58649698e-01, -4.49256152e-01,  7.60325938e-02,\n",
              "        7.82393478e-03, -1.83271438e-01,  7.61179850e-02, -3.11659835e-02,\n",
              "        3.35486680e-01,  7.93366432e-02,  4.73652393e-01,  4.46648985e-01,\n",
              "       -1.53591901e-01,  1.56724945e-01, -4.99867022e-01, -3.92111152e-01,\n",
              "       -1.84941530e-01, -4.12973255e-01, -2.37181738e-01, -2.81259995e-02,\n",
              "       -2.13064238e-01, -2.61592746e-01,  2.59780558e-03,  2.50462264e-01,\n",
              "        6.80215478e-01,  1.50102481e-01,  2.85195503e-02,  9.35437307e-02,\n",
              "        1.84929386e-01,  9.14610252e-02, -4.29814726e-01, -2.06139803e-01,\n",
              "        5.13771534e-01,  3.49205762e-01, -9.37306285e-02,  6.19348101e-02,\n",
              "        2.89960772e-01, -7.75189459e-01, -4.38939512e-01, -2.68546700e-01,\n",
              "        3.43966693e-01, -3.91728193e-01, -3.66334885e-01, -2.06267786e+00,\n",
              "        3.54025304e-01, -2.32063487e-01,  1.18795976e-01,  9.11578164e-02,\n",
              "       -4.45042178e-02,  6.48082674e-01, -4.88308161e-01,  1.15792060e+00,\n",
              "       -1.12572379e-01,  6.17832065e-01, -2.59146780e-01, -2.05977678e-01,\n",
              "        1.58557758e-01, -2.35605881e-01,  9.24473703e-01, -5.40670335e-01,\n",
              "        3.63959730e-01, -1.99178189e-01, -2.54377037e-01, -3.13268036e-01,\n",
              "        4.84224051e-01,  4.12086338e-01, -5.22650242e-01, -4.33199912e-01,\n",
              "       -1.40279338e-01,  1.69145608e+00,  6.67690396e-01, -5.53603768e-01,\n",
              "       -2.02356175e-01,  1.20888092e-01,  8.94589350e-02, -2.55839914e-01,\n",
              "       -8.68380904e-01,  4.49038297e-01,  5.10505199e-01,  3.50582153e-01,\n",
              "        1.16515331e-01, -5.31371891e-01,  8.41718763e-02, -1.64995432e-01,\n",
              "        3.06182772e-01,  1.93302900e-01, -5.47745228e-01,  3.11293453e-01,\n",
              "        1.33586824e-01,  1.39706522e-01,  1.69564486e-01, -2.84256160e-01,\n",
              "        7.00225294e-01,  3.71134341e-01, -3.08432113e-02,  8.53812322e-02,\n",
              "        1.48865432e-01, -2.01032743e-01, -5.23165390e-02, -6.46536708e-01,\n",
              "       -1.00295924e-01, -3.96646500e-01,  4.16183323e-01, -3.96808892e-01,\n",
              "       -5.41963220e-01,  4.92899984e-01, -5.22793889e-01,  3.05025965e-01,\n",
              "        2.56544828e-01, -4.93476331e-01, -6.26624703e-01,  5.31525493e-01,\n",
              "       -6.42894566e-01,  3.59747916e-01,  6.02005303e-01,  2.70486306e-02,\n",
              "       -3.59960526e-01,  2.87779719e-01,  1.76504344e-01,  7.74845064e-01,\n",
              "        1.31816164e-01, -1.15222752e-01,  2.89757252e-01, -1.22938238e-01,\n",
              "        1.88296407e-01,  8.38460922e-02,  1.65966123e-01,  5.67074656e-01,\n",
              "        1.92509040e-01, -2.91634291e-01, -4.86389339e-01,  1.84493169e-01,\n",
              "       -2.33239412e-01, -5.99977153e-04, -2.55426943e-01, -4.78800774e-01,\n",
              "        2.05425769e-01, -7.65041113e-02,  9.37221870e-02, -2.66939211e+00,\n",
              "        5.21396279e-01,  1.46102384e-01,  4.01676953e-01, -3.04001510e-01,\n",
              "       -3.81737769e-01,  5.36202192e-01,  7.93129429e-02, -6.82779908e-01,\n",
              "       -7.47247273e-03,  2.82108754e-01,  2.85036474e-01,  4.31209207e-02,\n",
              "       -5.72883546e-01,  1.07338913e-01, -2.84052361e-02,  5.19254386e-01,\n",
              "       -7.53725052e-01, -2.14575157e-01, -4.44067642e-02,  8.13205391e-02,\n",
              "       -6.34386688e-02,  1.49462247e+00, -2.90646821e-01,  1.22443698e-01,\n",
              "        3.60910624e-01,  2.53312141e-01,  4.04749453e-01, -3.24389189e-01,\n",
              "        1.33693740e-01,  5.91144562e-01, -2.81902552e-01,  5.15593998e-02,\n",
              "       -2.10034903e-02,  1.63217396e-01,  1.00044167e+00, -3.05099458e-01,\n",
              "        6.42248243e-02,  7.24191889e-02, -4.18574005e-01, -3.70032072e-01,\n",
              "        2.59639025e-01, -5.30680642e-02, -1.38728827e-01,  6.45690322e-01,\n",
              "       -3.00850660e-01,  5.25382422e-02,  2.36614898e-01,  2.84449710e-03,\n",
              "       -3.67926180e-01, -4.58991468e-01, -6.45526707e-01, -5.16101003e-01,\n",
              "       -1.76004261e-01,  2.99382895e-01,  2.80906439e-01, -4.94163066e-01,\n",
              "        1.49919614e-01,  1.75864249e-01, -4.26869355e-02,  5.34018204e-02,\n",
              "       -3.09824407e-01, -3.88418809e-02,  6.71498999e-02, -1.79800823e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rjKvTKkLTg0-"
      },
      "outputs": [],
      "source": [
        "def generate_answer(question, context):\n",
        "    start_time = time.time()\n",
        "    prompt = prompt_template.format(context=context, question=question)\n",
        "    checkpoint_1 = time.time() - start_time\n",
        "    print(f\"Checkpoint 1: {checkpoint_1:.4f} seconds\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    generator = HuggingFaceLocalGenerator(model=\"HuggingFaceTB/SmolLM-1.7B-Instruct\",\n",
        "                                      task=\"text-generation\",\n",
        "                                      generation_kwargs={\n",
        "                                        \"max_new_tokens\": 150,\n",
        "                                        \"do_sample\": False,\n",
        "                                      })\n",
        "    checkpoint_2 = time.time() - start_time\n",
        "    print(f\"Checkpoint 2: {checkpoint_2:.4f} seconds\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    generator.warm_up()\n",
        "    checkpoint_3 = time.time() - start_time\n",
        "    print(f\"Checkpoint 3: {checkpoint_3:.4f} seconds\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    response = generator.run(prompt)\n",
        "    checkpoint_4 = time.time() - start_time\n",
        "    print(f\"Checkpoint 4: {checkpoint_4:.4f} seconds\")\n",
        "\n",
        "    replies = response['replies']\n",
        "\n",
        "    generate_time = sum([checkpoint_1, checkpoint_2, checkpoint_3, checkpoint_4])\n",
        "    print(f\"Answer generation time: {generate_time:.4f} seconds\")\n",
        "    return replies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nV_ug5MMT-6c"
      },
      "outputs": [],
      "source": [
        "def answer_question(question, embeddings):\n",
        "    answer_start = time.time()\n",
        "    context = get_relevant_context(question, embeddings)\n",
        "    context_text = \" \".join(context[0])\n",
        "    answer = generate_answer(question, context_text)\n",
        "    answer_time = time.time() - answer_start\n",
        "    print(f\"Total time: {answer_time:.4f} seconds\")\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# colab\n",
        "question = \"What does The Colossus of Rhodes Statue look like?\"\n",
        "answer = answer_question(question, embeddings)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_ftAOy-t5Te",
        "outputId": "5de4be77-8953-4583-90ea-dafef6fa733e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context retrieval time: 0.0328 seconds\n",
            "Checkpoint 1: 0.0000 seconds\n",
            "Checkpoint 2: 0.0005 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint 3: 16.5065 seconds\n",
            "Checkpoint 4: 7.5763 seconds\n",
            "Answer generation time: 24.0833 seconds\n",
            "Total time: 24.1166 seconds\n",
            "['\\nThe Colossus of Rhodes Statue is a massive bronze statue of the Greek god Helios, located in the city of Rhodes, Greece. It is one of the Seven Wonders of the Ancient World and is considered one of the largest statues of the ancient world. The statue is over 100 feet (30 meters) tall and weighs around 100 tons. It is made of bronze and is adorned with 230 marble and alabaster columns. The statue is located on the island of Rhodes, which was an important center of trade and commerce in the ancient world.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNZo65D4UGTE",
        "outputId": "546e5b19-6f59-40c7-a0da-cd5d97de006c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context retrieval time: 0.0222 seconds\n",
            "Checkpoint 1: 0.0000 seconds\n",
            "Checkpoint 2: 0.0008 seconds\n",
            "Checkpoint 3: 7.8145 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint 4: 68.4128 seconds\n",
            "Answer generation time: 76.2282 seconds\n",
            "Total time: 76.2525 seconds\n",
            "['\\nThe Colossus of Rhodes Statue is a massive bronze statue of the Greek god Helios, located in the city of Rhodes, Greece. It is one of the Seven Wonders of the Ancient World and is considered one of the largest statues of the ancient world. The statue is over 100 feet (30 meters) tall and weighs around 100 tons. It is made of bronze and is adorned with 230 marble and alabaster columns. The statue is located on the island of Rhodes, which was an important center of trade and commerce in the ancient world.']\n"
          ]
        }
      ],
      "source": [
        "# lightning\n",
        "question = \"What does The Colossus of Rhodes Statue look like?\"\n",
        "answer = answer_question(question, embeddings)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvfXEH5hqyAJ",
        "outputId": "991653bc-8fb4-4bdf-ef0a-0f531ba3bca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context retrieval time: 0.0450 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['The Colossus of Rhodes (Ancient Greek: ὁ Κολοσσὸς Ῥόδιος, romanized:\\xa0ho Kolossòs Rhódios Greek: Κολοσσός της Ρόδου, romanized:\\xa0Kolossós tes Rhódou)[a] was a statue of the Greek sun-god Helios, erected in the city of Rhodes, on the Greek island of the same name, by Chares of Lindos in 280\\xa0BC. One of the Seven Wonders of the Ancient World, it was constructed to celebrate the successful defence of Rhodes city against an attack by Demetrius Poliorcetes, who had besieged it for a year with a large army and navy.\\nAccording to most contemporary descriptions, the Colossus stood approximately 70 cubits, or 33 metres (108 feet) high – approximately the height of the modern Statue of Liberty from feet to crown – making it the tallest statue in the ancient world.[2] It collapsed during the earthquake of 226 BC, although parts of it were preserved. In accordance with a certain oracle, the Rhodians did not build it again.[3] John Malalas wrote that Hadrian in his reign re-erected the Colossus,[4] but he was mistaken.[5] According to the Suda, the Rhodians were called Colossaeans (Κολοσσαεῖς), because they erected the statue on the island.',\n",
              " \"Construction[edit]\\nTimeline and map of the Seven Wonders of the Ancient World, including the Colossus of Rhodes\\nConstruction began in 292\\xa0BC. Ancient accounts, which differ to some degree, describe the structure as being built with iron tie bars to which brass plates were fixed to form the skin. The interior of the structure, which stood on a 15-metre-high (49-foot) white marble pedestal near the Rhodes harbour entrance, was then filled with stone blocks as construction progressed.[14] Other sources place the Colossus on a breakwater in the harbour. According to most contemporary descriptions, the statue itself was about 70 cubits, or 32 metres (105 feet) tall.[15] Much of the iron and bronze was reforged from the various weapons Demetrius's army left behind, and the abandoned second siege tower may have been used for scaffolding around the lower levels during construction.\\n\",\n",
              " 'Within it, too, are to be seen large masses of rock, by the weight of which the artist steadied it while erecting it.[22][23]\\nDestruction of the remains[edit]\\nThe ultimate fate of the remains of the statue is uncertain. Rhodes has two serious earthquakes per century, owing to its location on the seismically unstable Hellenic Arc. Pausanias tells us, writing ca. 174, how the city was so devastated by an earthquake that the Sibyl oracle foretelling its destruction was considered fulfilled.[24] This means the statue could not have survived for long if it was ever repaired. By the 4th century Rhodes was Christianized, meaning any further maintenance or rebuilding, if there ever was any before, on an ancient pagan statue is unlikely. The metal would have likely been used for coins and maybe also tools by the time of the Arab wars, especially during earlier conflicts such as the Sassanian wars.[9]\\nThe onset of Islamic naval incursions against the Byzantine empire gave rise to a dramatic account of what became of the Colossus. ',\n",
              " '[6]\\nIn 653, an Arab force under Muslim general Muawiyah I conquered Rhodes, and according to the Chronicle of Theophanes the Confessor,[7] the statue was completely destroyed and the remains sold;[8] this account may be unreliable.[9]\\nSince 2008, a series of as-yet-unrealized proposals to build a new Colossus at Rhodes Harbour have been announced, although the actual location of the original monument remains in dispute.[10][11]\\n\\nSiege of Rhodes[edit]\\nMain article: Siege of Rhodes (305–304\\xa0BC)\\nIn the early fourth century BC, Rhodes, allied with Ptolemy I of Egypt, prevented a mass invasion staged by their common enemy, Antigonus I Monophthalmus.\\nIn 304\\xa0BC a relief force of ships sent by Ptolemy arrived, and Demetrius (son of Antigonus) and his army abandoned the siege, leaving behind most of their siege equipment. To celebrate their victory, the Rhodians sold the equipment left behind for 300 talents[12] and decided to use the money to build a colossal statue of their patron god, Helios. Construction was left to the direction of Chares, a native of Lindos in Rhodes, who had been involved with large-scale statues before. His teacher, the sculptor Lysippos, had constructed a 22-metre-high (72-foot)[13] bronze statue of Zeus at Tarentum.\\n\\n',\n",
              " '[28]\\nGiven the likely previous neglect of the remains and various opportunities for authorities to have repurposed the metal, as well as the fact that, Islamic incursions notwithstanding, the island remained an important Byzantine strategic point well into the ninth century, an Arabic raid is unlikely to have found much, if any, remaining metal to carry away. For these reasons, as well as the negative perception of the Arab conquests, L. I. Conrad considers Theophanes\\' story of the dismantling of the statue as likely propaganda, like the destruction of the Library of Alexandria.[9]\\n\\nPosture[edit]\\nThe Colossus as imagined in a 16th-century engraving by Martin Heemskerck, part of his series of the Seven Wonders of the World\\nThe harbour-straddling Colossus was a figment of medieval imaginations based on the dedication text\\'s mention of \"over land and sea\" twice and the writings of an Italian visitor who in 1395 noted that local tradition held that the right foot had stood where the church of St John of the Colossus was then located.[29] Many later illustrations show the statue with one foot on either side of the harbour mouth with ships passing under it. ']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_relevant_context(question, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaA4LSwhs4FG",
        "outputId": "1fdfbe9f-5dd4-4dd2-c9bb-d1156dc2d6d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context retrieval time: 0.0170 seconds\n",
            "Checkpoint 1: 0.0000 seconds\n",
            "Checkpoint 2: 0.0003 seconds\n",
            "Checkpoint 3: 9.3937 seconds\n",
            "Checkpoint 4: 76.4054 seconds\n",
            "Answer generation time: 85.7994 seconds\n",
            "Total time: 85.8186 seconds\n",
            "[\"\\nThe Library of Alexandria was a major center of learning and scholarship in ancient Alexandria, Egypt. It was founded in the 3rd century BC and was destroyed by fire in the 5th century AD. The library was a collection of manuscripts and scrolls containing works of Greek philosophers, scientists, and mathematicians, as well as texts on mathematics, astronomy, and medicine. The library was also a center of learning for the study of philosophy, mathematics, and astronomy.\\n\\nThe library was founded by Ptolemy I Soter, one of Alexander the Great's generals, and was built on the site of an earlier temple dedicated to the Muses. The library was a major center of learning and scholarship, attracting scholars from all over the Mediterranean world. It\"]\n"
          ]
        }
      ],
      "source": [
        "question = \"What is the history of the Library of Alexandria\"\n",
        "answer = answer_question(question, embeddings)\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}